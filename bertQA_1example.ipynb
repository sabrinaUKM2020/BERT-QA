{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "bertQA-1example.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ux2IciTGEvXA"
      },
      "source": [
        "https://mccormickml.com/2020/03/10/question-answering-with-a-fine-tuned-BERT/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UzJD05NkHjjP"
      },
      "source": [
        "1. install transformer - to get pre-trained model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hCnQPVElEwHW",
        "outputId": "17e47d70-96c2-48d0-a807-6058feab660c"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.4.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (20.9)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.43)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.1)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers) (3.7.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2020.12.5)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yE7euE_vE19-"
      },
      "source": [
        "import torch"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J2GziJ1IE7ir"
      },
      "source": [
        "from transformers import BertForQuestionAnswering\n",
        "\n",
        "model = BertForQuestionAnswering.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')\n"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t3dWy844HrWM"
      },
      "source": [
        "2. import BERT QA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K2jdwhGzFZrv"
      },
      "source": [
        "from transformers import BertTokenizer\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E4quOgPmFfed",
        "outputId": "b9001a4b-8e24-40e7-8760-77e1527b7aca"
      },
      "source": [
        "question = \"How quickly could COVID-19 vaccines stop the pandemic?\"\n",
        "answer_text = ''' The impact of COVID-19 vaccines on the pandemic will depend on several factors. These include the effectiveness of the vaccines; how quickly they are approved, manufactured, and delivered; the possible development of other variants and how many people get vaccinated\n",
        "Whilst trials have shown several COVID-19 vaccines to have high levels of efficacy, like all other vaccines, COVID-19 vaccines will not be 100% effective. WHO is working to help ensure that approved vaccines are as effective as possible, so they can have the greatest impact on the pandemic. '''\n",
        "            \n",
        "encoding = tokenizer.encode_plus(text=question,text_pair=paragraph, add_special=True)\n",
        "\n",
        "inputs = encoding['input_ids']  #Token embeddings\n",
        "sentence_embedding = encoding['token_type_ids']  #Segment embeddings\n",
        "tokens = tokenizer.convert_ids_to_tokens(inputs) #input tokens\n"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Keyword arguments {'add_special': True} not recognized.\n",
            "Keyword arguments {'add_special': True} not recognized.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "37NekOfcFl87",
        "outputId": "10d48b10-e41f-446f-8557-d897700f958a"
      },
      "source": [
        "# Apply the tokenizer to the input text, treating them as a text-pair.\n",
        "input_ids = tokenizer.encode(question, answer_text)\n",
        "\n",
        "print('The input has a total of {:} tokens.'.format(len(input_ids)))"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The input has a total of 131 tokens.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jn3-C3MFF0iS",
        "outputId": "03ad2f6b-53a7-4f22-8b8e-d86d3d0e9388"
      },
      "source": [
        "# BERT only needs the token IDs, but for the purpose of inspecting the \n",
        "# tokenizer's behavior, let's also get the token strings and display them.\n",
        "tokens = tokenizer.convert_ids_to_tokens(input_ids)\n",
        "\n",
        "# For each token and its id...\n",
        "for token, id in zip(tokens, input_ids):\n",
        "    \n",
        "    # If this is the [SEP] token, add some space around it to make it stand out.\n",
        "    if id == tokenizer.sep_token_id:\n",
        "        print('')\n",
        "    \n",
        "    # Print the token string and its ID in two columns.\n",
        "    print('{:<12} {:>6,}'.format(token, id))\n",
        "\n",
        "    if id == tokenizer.sep_token_id:\n",
        "        print('')\n",
        "    "
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[CLS]           101\n",
            "how           2,129\n",
            "quickly       2,855\n",
            "could         2,071\n",
            "co            2,522\n",
            "##vid        17,258\n",
            "-             1,011\n",
            "19            2,539\n",
            "vaccines     28,896\n",
            "stop          2,644\n",
            "the           1,996\n",
            "pan           6,090\n",
            "##de          3,207\n",
            "##mic         7,712\n",
            "?             1,029\n",
            "\n",
            "[SEP]           102\n",
            "\n",
            "the           1,996\n",
            "impact        4,254\n",
            "of            1,997\n",
            "co            2,522\n",
            "##vid        17,258\n",
            "-             1,011\n",
            "19            2,539\n",
            "vaccines     28,896\n",
            "on            2,006\n",
            "the           1,996\n",
            "pan           6,090\n",
            "##de          3,207\n",
            "##mic         7,712\n",
            "will          2,097\n",
            "depend       12,530\n",
            "on            2,006\n",
            "several       2,195\n",
            "factors       5,876\n",
            ".             1,012\n",
            "these         2,122\n",
            "include       2,421\n",
            "the           1,996\n",
            "effectiveness 12,353\n",
            "of            1,997\n",
            "the           1,996\n",
            "vaccines     28,896\n",
            ";             1,025\n",
            "how           2,129\n",
            "quickly       2,855\n",
            "they          2,027\n",
            "are           2,024\n",
            "approved      4,844\n",
            ",             1,010\n",
            "manufactured  7,609\n",
            ",             1,010\n",
            "and           1,998\n",
            "delivered     5,359\n",
            ";             1,025\n",
            "the           1,996\n",
            "possible      2,825\n",
            "development   2,458\n",
            "of            1,997\n",
            "other         2,060\n",
            "variants     10,176\n",
            "and           1,998\n",
            "how           2,129\n",
            "many          2,116\n",
            "people        2,111\n",
            "get           2,131\n",
            "va           12,436\n",
            "##cci        14,693\n",
            "##nated      23,854\n",
            "whilst        5,819\n",
            "trials        7,012\n",
            "have          2,031\n",
            "shown         3,491\n",
            "several       2,195\n",
            "co            2,522\n",
            "##vid        17,258\n",
            "-             1,011\n",
            "19            2,539\n",
            "vaccines     28,896\n",
            "to            2,000\n",
            "have          2,031\n",
            "high          2,152\n",
            "levels        3,798\n",
            "of            1,997\n",
            "efficacy     21,150\n",
            ",             1,010\n",
            "like          2,066\n",
            "all           2,035\n",
            "other         2,060\n",
            "vaccines     28,896\n",
            ",             1,010\n",
            "co            2,522\n",
            "##vid        17,258\n",
            "-             1,011\n",
            "19            2,539\n",
            "vaccines     28,896\n",
            "will          2,097\n",
            "not           2,025\n",
            "be            2,022\n",
            "100           2,531\n",
            "%             1,003\n",
            "effective     4,621\n",
            ".             1,012\n",
            "who           2,040\n",
            "is            2,003\n",
            "working       2,551\n",
            "to            2,000\n",
            "help          2,393\n",
            "ensure        5,676\n",
            "that          2,008\n",
            "approved      4,844\n",
            "vaccines     28,896\n",
            "are           2,024\n",
            "as            2,004\n",
            "effective     4,621\n",
            "as            2,004\n",
            "possible      2,825\n",
            ",             1,010\n",
            "so            2,061\n",
            "they          2,027\n",
            "can           2,064\n",
            "have          2,031\n",
            "the           1,996\n",
            "greatest      4,602\n",
            "impact        4,254\n",
            "on            2,006\n",
            "the           1,996\n",
            "pan           6,090\n",
            "##de          3,207\n",
            "##mic         7,712\n",
            ".             1,012\n",
            "\n",
            "[SEP]           102\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iW8xOPhfF2UJ"
      },
      "source": [
        "# Search the input_ids for the first instance of the `[SEP]` token.\n",
        "sep_index = input_ids.index(tokenizer.sep_token_id)\n",
        "\n",
        "# The number of segment A tokens includes the [SEP] token istelf.\n",
        "num_seg_a = sep_index + 1\n",
        "\n",
        "# The remainder are segment B.\n",
        "num_seg_b = len(input_ids) - num_seg_a\n",
        "\n",
        "# Construct the list of 0s and 1s.\n",
        "segment_ids = [0]*num_seg_a + [1]*num_seg_b\n",
        "\n",
        "# There should be a segment_id for every input token.\n",
        "assert len(segment_ids) == len(input_ids)"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5PbcRRIvGR7p"
      },
      "source": [
        "# Run our example through the model.\n",
        "start_scores, end_scores = model(torch.tensor([input_ids]), # The tokens representing our input text.\n",
        "                                 token_type_ids=torch.tensor([segment_ids]),return_dict=False ) # The segment IDs to differentiate question from answer_text\n",
        "                                 "
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DeCnaV9oGYcf",
        "outputId": "d7fb5d96-3160-4e56-d618-102d0e4d5fde"
      },
      "source": [
        "# Find the tokens with the highest `start` and `end` scores.\n",
        "answer_start = torch.argmax(start_scores)\n",
        "answer_end = torch.argmax(end_scores)\n",
        "\n",
        "# Combine the tokens in the answer and print it out.\n",
        "answer = ' '.join(tokens[answer_start:answer_end+1])\n",
        "\n",
        "print('Answer: \"' + answer + '\"')"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Answer: \"how quickly they are approved , manufactured , and delivered\"\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "obUogkVLIWlU",
        "outputId": "5eb240bd-0373-4f64-af37-2254f10ed3ab"
      },
      "source": [
        "corrected_answer = ''\n",
        "\n",
        "for word in answer.split():\n",
        "    \n",
        "    #If it's a subword token\n",
        "    if word[0:2] == '##':\n",
        "        corrected_answer += word[2:]\n",
        "    else:\n",
        "        corrected_answer += ' ' + word\n",
        "print('Question = How quickly could COVID-19 vaccines stop the pandemic?')\n",
        "print('Corrected Answer:'+corrected_answer)"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Question = How quickly could COVID-19 vaccines stop the pandemic?\n",
            "Corrected Answer: how quickly they are approved , manufactured , and delivered\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}