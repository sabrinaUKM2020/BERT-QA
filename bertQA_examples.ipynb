{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "bertQA-1example.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ux2IciTGEvXA"
      },
      "source": [
        "NOTE: This code is taken from  \n",
        "https://mccormickml.com/2020/03/10/question-answering-with-a-fine-tuned-BERT/ with some modifications. FYI, these codes are only for my learning purpose, Use it "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UzJD05NkHjjP"
      },
      "source": [
        "1. install transformer - to get pre-trained model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hCnQPVElEwHW",
        "outputId": "17e47d70-96c2-48d0-a807-6058feab660c"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.4.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (20.9)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.43)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.1)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers) (3.7.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2020.12.5)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yE7euE_vE19-"
      },
      "source": [
        "import torch"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J2GziJ1IE7ir"
      },
      "source": [
        "from transformers import BertForQuestionAnswering\n",
        "\n",
        "model = BertForQuestionAnswering.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')\n"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t3dWy844HrWM"
      },
      "source": [
        "2. import BERT QA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K2jdwhGzFZrv"
      },
      "source": [
        "from transformers import BertTokenizer\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E4quOgPmFfed",
        "outputId": "b9001a4b-8e24-40e7-8760-77e1527b7aca"
      },
      "source": [
        "question = \"How quickly could COVID-19 vaccines stop the pandemic?\"\n",
        "answer_text = ''' The impact of COVID-19 vaccines on the pandemic will depend on several factors. These include the effectiveness of the vaccines; how quickly they are approved, manufactured, and delivered; the possible development of other variants and how many people get vaccinated\n",
        "Whilst trials have shown several COVID-19 vaccines to have high levels of efficacy, like all other vaccines, COVID-19 vaccines will not be 100% effective. WHO is working to help ensure that approved vaccines are as effective as possible, so they can have the greatest impact on the pandemic. '''\n",
        "            \n",
        "encoding = tokenizer.encode_plus(text=question,text_pair=paragraph, add_special=True)\n",
        "\n",
        "inputs = encoding['input_ids']  #Token embeddings\n",
        "sentence_embedding = encoding['token_type_ids']  #Segment embeddings\n",
        "tokens = tokenizer.convert_ids_to_tokens(inputs) #input tokens\n"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Keyword arguments {'add_special': True} not recognized.\n",
            "Keyword arguments {'add_special': True} not recognized.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "37NekOfcFl87",
        "outputId": "10d48b10-e41f-446f-8557-d897700f958a"
      },
      "source": [
        "# Apply the tokenizer to the input text, treating them as a text-pair.\n",
        "input_ids = tokenizer.encode(question, answer_text)\n",
        "\n",
        "print('The input has a total of {:} tokens.'.format(len(input_ids)))"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The input has a total of 131 tokens.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jn3-C3MFF0iS",
        "outputId": "03ad2f6b-53a7-4f22-8b8e-d86d3d0e9388"
      },
      "source": [
        "# BERT only needs the token IDs, but for the purpose of inspecting the \n",
        "# tokenizer's behavior, let's also get the token strings and display them.\n",
        "tokens = tokenizer.convert_ids_to_tokens(input_ids)\n",
        "\n",
        "# For each token and its id...\n",
        "for token, id in zip(tokens, input_ids):\n",
        "    \n",
        "    # If this is the [SEP] token, add some space around it to make it stand out.\n",
        "    if id == tokenizer.sep_token_id:\n",
        "        print('')\n",
        "    \n",
        "    # Print the token string and its ID in two columns.\n",
        "    print('{:<12} {:>6,}'.format(token, id))\n",
        "\n",
        "    if id == tokenizer.sep_token_id:\n",
        "        print('')\n",
        "    "
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[CLS]           101\n",
            "how           2,129\n",
            "quickly       2,855\n",
            "could         2,071\n",
            "co            2,522\n",
            "##vid        17,258\n",
            "-             1,011\n",
            "19            2,539\n",
            "vaccines     28,896\n",
            "stop          2,644\n",
            "the           1,996\n",
            "pan           6,090\n",
            "##de          3,207\n",
            "##mic         7,712\n",
            "?             1,029\n",
            "\n",
            "[SEP]           102\n",
            "\n",
            "the           1,996\n",
            "impact        4,254\n",
            "of            1,997\n",
            "co            2,522\n",
            "##vid        17,258\n",
            "-             1,011\n",
            "19            2,539\n",
            "vaccines     28,896\n",
            "on            2,006\n",
            "the           1,996\n",
            "pan           6,090\n",
            "##de          3,207\n",
            "##mic         7,712\n",
            "will          2,097\n",
            "depend       12,530\n",
            "on            2,006\n",
            "several       2,195\n",
            "factors       5,876\n",
            ".             1,012\n",
            "these         2,122\n",
            "include       2,421\n",
            "the           1,996\n",
            "effectiveness 12,353\n",
            "of            1,997\n",
            "the           1,996\n",
            "vaccines     28,896\n",
            ";             1,025\n",
            "how           2,129\n",
            "quickly       2,855\n",
            "they          2,027\n",
            "are           2,024\n",
            "approved      4,844\n",
            ",             1,010\n",
            "manufactured  7,609\n",
            ",             1,010\n",
            "and           1,998\n",
            "delivered     5,359\n",
            ";             1,025\n",
            "the           1,996\n",
            "possible      2,825\n",
            "development   2,458\n",
            "of            1,997\n",
            "other         2,060\n",
            "variants     10,176\n",
            "and           1,998\n",
            "how           2,129\n",
            "many          2,116\n",
            "people        2,111\n",
            "get           2,131\n",
            "va           12,436\n",
            "##cci        14,693\n",
            "##nated      23,854\n",
            "whilst        5,819\n",
            "trials        7,012\n",
            "have          2,031\n",
            "shown         3,491\n",
            "several       2,195\n",
            "co            2,522\n",
            "##vid        17,258\n",
            "-             1,011\n",
            "19            2,539\n",
            "vaccines     28,896\n",
            "to            2,000\n",
            "have          2,031\n",
            "high          2,152\n",
            "levels        3,798\n",
            "of            1,997\n",
            "efficacy     21,150\n",
            ",             1,010\n",
            "like          2,066\n",
            "all           2,035\n",
            "other         2,060\n",
            "vaccines     28,896\n",
            ",             1,010\n",
            "co            2,522\n",
            "##vid        17,258\n",
            "-             1,011\n",
            "19            2,539\n",
            "vaccines     28,896\n",
            "will          2,097\n",
            "not           2,025\n",
            "be            2,022\n",
            "100           2,531\n",
            "%             1,003\n",
            "effective     4,621\n",
            ".             1,012\n",
            "who           2,040\n",
            "is            2,003\n",
            "working       2,551\n",
            "to            2,000\n",
            "help          2,393\n",
            "ensure        5,676\n",
            "that          2,008\n",
            "approved      4,844\n",
            "vaccines     28,896\n",
            "are           2,024\n",
            "as            2,004\n",
            "effective     4,621\n",
            "as            2,004\n",
            "possible      2,825\n",
            ",             1,010\n",
            "so            2,061\n",
            "they          2,027\n",
            "can           2,064\n",
            "have          2,031\n",
            "the           1,996\n",
            "greatest      4,602\n",
            "impact        4,254\n",
            "on            2,006\n",
            "the           1,996\n",
            "pan           6,090\n",
            "##de          3,207\n",
            "##mic         7,712\n",
            ".             1,012\n",
            "\n",
            "[SEP]           102\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iW8xOPhfF2UJ"
      },
      "source": [
        "# Search the input_ids for the first instance of the `[SEP]` token.\n",
        "sep_index = input_ids.index(tokenizer.sep_token_id)\n",
        "\n",
        "# The number of segment A tokens includes the [SEP] token istelf.\n",
        "num_seg_a = sep_index + 1\n",
        "\n",
        "# The remainder are segment B.\n",
        "num_seg_b = len(input_ids) - num_seg_a\n",
        "\n",
        "# Construct the list of 0s and 1s.\n",
        "segment_ids = [0]*num_seg_a + [1]*num_seg_b\n",
        "\n",
        "# There should be a segment_id for every input token.\n",
        "assert len(segment_ids) == len(input_ids)"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5PbcRRIvGR7p"
      },
      "source": [
        "# Run our example through the model.\n",
        "start_scores, end_scores = model(torch.tensor([input_ids]), # The tokens representing our input text.\n",
        "                                 token_type_ids=torch.tensor([segment_ids]),return_dict=False ) # The segment IDs to differentiate question from answer_text\n",
        "                                 "
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DeCnaV9oGYcf",
        "outputId": "d7fb5d96-3160-4e56-d618-102d0e4d5fde"
      },
      "source": [
        "# Find the tokens with the highest `start` and `end` scores.\n",
        "answer_start = torch.argmax(start_scores)\n",
        "answer_end = torch.argmax(end_scores)\n",
        "\n",
        "# Combine the tokens in the answer and print it out.\n",
        "answer = ' '.join(tokens[answer_start:answer_end+1])\n",
        "\n",
        "print('Answer: \"' + answer + '\"')"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Answer: \"how quickly they are approved , manufactured , and delivered\"\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "obUogkVLIWlU",
        "outputId": "5eb240bd-0373-4f64-af37-2254f10ed3ab"
      },
      "source": [
        "corrected_answer = ''\n",
        "\n",
        "for word in answer.split():\n",
        "    \n",
        "    #If it's a subword token\n",
        "    if word[0:2] == '##':\n",
        "        corrected_answer += word[2:]\n",
        "    else:\n",
        "        corrected_answer += ' ' + word\n",
        "print('Question = How quickly could COVID-19 vaccines stop the pandemic?')\n",
        "print('Corrected Answer:'+corrected_answer)"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Question = How quickly could COVID-19 vaccines stop the pandemic?\n",
            "Corrected Answer: how quickly they are approved , manufactured , and delivered\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gOQ_nGnDO9wz"
      },
      "source": [
        "BERT QA with more than one example"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0B5waZ78PGTc"
      },
      "source": [
        "def answer_question(question, answer_text):\n",
        "    '''\n",
        "    Takes a `question` string and an `answer_text` string (which contains the\n",
        "    answer), and identifies the words within the `answer_text` that are the\n",
        "    answer. Prints them out.\n",
        "    '''\n",
        "    # ======== Tokenize ========\n",
        "    # Apply the tokenizer to the input text, treating them as a text-pair.\n",
        "    input_ids = tokenizer.encode(question, answer_text)\n",
        "\n",
        "    # Report how long the input sequence is.\n",
        "    print('Query has {:,} tokens.\\n'.format(len(input_ids)))\n",
        "\n",
        "    # ======== Set Segment IDs ========\n",
        "    # Search the input_ids for the first instance of the `[SEP]` token.\n",
        "    sep_index = input_ids.index(tokenizer.sep_token_id)\n",
        "\n",
        "    # The number of segment A tokens includes the [SEP] token istelf.\n",
        "    num_seg_a = sep_index + 1\n",
        "\n",
        "    # The remainder are segment B.\n",
        "    num_seg_b = len(input_ids) - num_seg_a\n",
        "\n",
        "    # Construct the list of 0s and 1s.\n",
        "    segment_ids = [0]*num_seg_a + [1]*num_seg_b\n",
        "\n",
        "    # There should be a segment_id for every input token.\n",
        "    assert len(segment_ids) == len(input_ids)\n",
        "\n",
        "    # ======== Evaluate ========\n",
        "    # Run our example question through the model.\n",
        "    start_scores, end_scores = model(torch.tensor([input_ids]), # The tokens representing our input text.\n",
        "                                    token_type_ids=torch.tensor([segment_ids]),return_dict=False) # The segment IDs to differentiate question from answer_text\n",
        "\n",
        "    # ======== Reconstruct Answer ========\n",
        "    # Find the tokens with the highest `start` and `end` scores.\n",
        "    answer_start = torch.argmax(start_scores)\n",
        "    answer_end = torch.argmax(end_scores)\n",
        "\n",
        "    # Get the string versions of the input tokens.\n",
        "    tokens = tokenizer.convert_ids_to_tokens(input_ids)\n",
        "\n",
        "    # Start with the first token.\n",
        "    answer = tokens[answer_start]\n",
        "\n",
        "    # Select the remaining answer tokens and join them with whitespace.\n",
        "    for i in range(answer_start + 1, answer_end + 1):\n",
        "        \n",
        "        # If it's a subword token, then recombine it with the previous token.\n",
        "        if tokens[i][0:2] == '##':\n",
        "            answer += tokens[i][2:]\n",
        "        \n",
        "        # Otherwise, add a space then the token.\n",
        "        else:\n",
        "            answer += ' ' + tokens[i]\n",
        "\n",
        "    print('Answer: \"' + answer + '\"')"
      ],
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1caqvYNyPOXg"
      },
      "source": [
        "start_scores, end_scores = model(input_ids=torch.tensor([inputs]), token_type_ids=torch.tensor([sentence_embedding]),return_dict=False)\n"
      ],
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o0QBbFzuPSlI"
      },
      "source": [
        "start_index = torch.argmax(start_scores)\n",
        "\n",
        "end_index = torch.argmax(end_scores)\n",
        "\n",
        "answer = ' '.join(tokens[start_index:end_index+1])"
      ],
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FZmdnPHSZ0cZ",
        "outputId": "ee32f98f-8db9-4aca-8ac7-ec423cb73298"
      },
      "source": [
        "import textwrap\n",
        "\n",
        "# Wrap text to 80 characters.\n",
        "wrapper = textwrap.TextWrapper(width=80) \n",
        "\n",
        "bert_abstract = \"We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be finetuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial taskspecific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).\"\n",
        "\n",
        "print(wrapper.fill(bert_abstract))"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "We introduce a new language representation model called BERT, which stands for\n",
            "Bidirectional Encoder Representations from Transformers. Unlike recent language\n",
            "representation models (Peters et al., 2018a; Radford et al., 2018), BERT is\n",
            "designed to pretrain deep bidirectional representations from unlabeled text by\n",
            "jointly conditioning on both left and right context in all layers. As a result,\n",
            "the pre-trained BERT model can be finetuned with just one additional output\n",
            "layer to create state-of-the-art models for a wide range of tasks, such as\n",
            "question answering and language inference, without substantial taskspecific\n",
            "architecture modifications. BERT is conceptually simple and empirically\n",
            "powerful. It obtains new state-of-the-art results on eleven natural language\n",
            "processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute\n",
            "improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1\n",
            "question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD\n",
            "v2.0 Test F1 to 83.1 (5.1 point absolute improvement).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FA48gtduZ2zP",
        "outputId": "a7ec33e8-79bf-423e-8a44-2a03bc72f7a6"
      },
      "source": [
        "question = \"What does the 'B' in BERT stand for?\"\n",
        "\n",
        "answer_question(question, bert_abstract)"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Query has 258 tokens.\n",
            "\n",
            "Answer: \"bidirectional encoder representations from transformers\"\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0WmBQW7WaL15",
        "outputId": "c12f6d72-e369-4510-dd72-f4594e5e67db"
      },
      "source": [
        "question = \"What are some example applications of BERT?\"\n",
        "\n",
        "answer_question(question, bert_abstract)"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Query has 255 tokens.\n",
            "\n",
            "Answer: \"question answering and language inference\"\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0pFUGL1ti3sA"
      },
      "source": [
        "question = \"What are some example applications of BERT?\"\n",
        "\n",
        "answer_question(question, bert_abstract)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}